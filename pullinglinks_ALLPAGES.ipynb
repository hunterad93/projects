{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "different strategy idea for getting listing urls in this notebook:\n",
    "\n",
    "This page has every craigslist website location: https://geo.craigslist.org/iso/us\n",
    "\n",
    "loop through all these with base query: <city>.craigslist.org/search/cta?bundleDuplicates=1&postedToday=1&purveyor=owner#search=1~gallery~0~0 (this is filtering to what was posted today and being sold by owner, but if we are running constantly this seems like the way?)\n",
    "\n",
    "within each loop, loop through all gallery~x~ to get all listings from pages after first page for that city\n",
    "\n",
    "create dictionary with url list as value per city key\n",
    "\n",
    "create a dictionary with df as value per city key\n",
    "\n",
    "I guess only real benefit of this approach is not having to worry about crafting search queries with make and model?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First cell gets a list of all craigslist cities and sets a base_query, search filters are part of URL, can customize further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://abilene.craigslist.org', 'https://akroncanton.craigslist.org', ... 'https://yubasutter.craigslist.org', 'https://yuma.craigslist.org', 'https://zanesville.craigslist.org']\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# URL for all Craigslist locations\n",
    "base_url = \"https://geo.craigslist.org/iso/us\"\n",
    "\n",
    "# Fetch the page with all Craigslist locations\n",
    "response = requests.get(base_url)\n",
    "\n",
    "# Parse the page with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the container with all city links\n",
    "container = soup.find('div', class_='geo-site-list-container')\n",
    "\n",
    "# Find all <a> elements within the container and extract hrefs (which should be URLs for Craigslist websites)\n",
    "city_urls = [element[\"href\"] for element in container.find_all(\"a\", href=True)]\n",
    "\n",
    "print(city_urls)\n",
    "\n",
    "# Query to append to each city URL\n",
    "base_query = \"/search/cta?bundleDuplicates=1&postedToday=1&purveyor=owner#search=1~gallery~0~0\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second cell defines scrape_pages function which uses selenium and loops through all pages of a city+base query combo, and adds all listing urls to a dictionary that would eventually include all cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_pages(base_url):\n",
    "    i = 0\n",
    "    all_urls = []\n",
    "\n",
    "    # Extract the city name from the base_url\n",
    "    city_name = base_url.split(\"//\")[1].split(\".\")[0]\n",
    "\n",
    "    # Set up Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "\n",
    "    # Set up the webdriver\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Append the page number to the URL\n",
    "            url = base_url + \"#search=1~gallery~\" + str(i) + \"~0\"\n",
    "            # Fetch the page\n",
    "            driver.get(url)\n",
    "            # Parse the page with BeautifulSoup\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "            # Find all <a> elements with class=\"titlestring\" and an href attribute\n",
    "            elements = soup.find_all(\"a\", class_=\"titlestring\", href=True)\n",
    "\n",
    "            # Extract the href attribute from each element and store them in the \"urls\" list\n",
    "            # Ignores ones that are in the 'search wider area' section of page by checking for city name\n",
    "            urls = [element[\"href\"] for element in elements if city_name in element[\"href\"]]\n",
    "\n",
    "            # Check if the newly scraped page has any new URLs\n",
    "            # This is because if you go beyond the limit of search results, it redirects to last page with actual results\n",
    "            if not urls or (set(urls).issubset(set(all_urls))):\n",
    "                break  # If not, break the loop\n",
    "\n",
    "            #Im betting these two tests are very very inefficient and should be revisited\n",
    "\n",
    "            # Extend the all_urls list with the URLs from this page\n",
    "            all_urls.extend(urls)\n",
    "\n",
    "            # Increment the page number\n",
    "            i += 1\n",
    "\n",
    "            # Sleep for a short period to avoid making too many requests in a short period of time\n",
    "            time.sleep(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "    # Quit the driver\n",
    "    driver.quit()\n",
    "\n",
    "    return all_urls\n",
    "\n",
    "\n",
    "# Dictionary to hold city names and corresponding URLs\n",
    "city_url_dict = {}\n",
    "\n",
    "# Loop through all Craigslist website URLs\n",
    "for city_url in city_urls:\n",
    "    # Extract the city name from the URL\n",
    "    city_name = city_url.split(\"//\")[1].split(\".\")[0]\n",
    "\n",
    "    # Append the base query to the city URL\n",
    "    base_query = city_url + \"/search/cta?bundleDuplicates=1&postedToday=1&purveyor=owner\"\n",
    "    \n",
    "    # Call the scrape_pages function and store the results in the dictionary\n",
    "    city_url_dict[city_name] = scrape_pages(base_query)\n",
    "\n",
    "# Print the dictionary\n",
    "for city, urls in city_url_dict.items():\n",
    "    print(f\"{city}: {urls}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here it is just code parsing listing page html mostly, this cell defines some functions for parsing different sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_name(name):\n",
    "    year_pattern = r'\\b\\d{4}\\b'\n",
    "    make_model_pattern = r'\\b([A-Za-z]+)\\s+([A-Za-z0-9-]+)'\n",
    "    \n",
    "    year_match = re.search(year_pattern, name)\n",
    "    make_model_match = re.search(make_model_pattern, name)\n",
    "\n",
    "    year = year_match.group(0) if year_match else None\n",
    "    make, model = make_model_match.groups() if make_model_match else (None, None)\n",
    "\n",
    "    return make, model, year\n",
    "\n",
    "def parse_attrgroup(soup):    \n",
    "    car_name = soup.find('p', class_='attrgroup')\n",
    "    if car_name is not None:\n",
    "        car_name = car_name.find('b').text.strip()\n",
    "    # Extract latitude and longitude\n",
    "    map_div = soup.find('div', {'id': 'map'})\n",
    "    latitude = float(map_div['data-latitude'])\n",
    "    longitude = float(map_div['data-longitude'])\n",
    "\n",
    "    # Extract attributes\n",
    "    attrgroup = soup.find_all('p', class_='attrgroup')\n",
    "    attributes = {}\n",
    "    for group in attrgroup:\n",
    "        for span in group.find_all('span'):\n",
    "            if ':' in span.text:\n",
    "                key, value = span.text.split(':')\n",
    "                attributes[key.strip()] = value.strip()\n",
    "\n",
    "    # Extract make, model, and year\n",
    "    make, model, year = parse_name(car_name)  # Pass the car_name variable to the parse_name function\n",
    "\n",
    "    parsed_data = {\n",
    "        'Title Status': attributes.get('title status'),\n",
    "        'Paint Color': attributes.get('paint color'),\n",
    "        'Odometer': int(attributes.get('odometer')),\n",
    "        'Drive': attributes.get('drive'),\n",
    "        'Condition': attributes.get('condition'),\n",
    "        'Make': make,\n",
    "        'Model': model,\n",
    "        'Year': year\n",
    "    }\n",
    "\n",
    "    return parsed_data\n",
    "\n",
    "def parse_ld_posting_data(soup):\n",
    "    script_tag = soup.find('script', {'id': 'ld_posting_data'})\n",
    "    json_data = json.loads(script_tag.string)\n",
    "    description = json_data['description']\n",
    "    price = json_data['offers']['price']\n",
    "    latitude = json_data['offers']['availableAtOrFrom']['geo']['latitude']\n",
    "    longitude = json_data['offers']['availableAtOrFrom']['geo']['longitude']\n",
    "\n",
    "    parsed_listing = {\n",
    "        'Price': price,\n",
    "        'Description': description,\n",
    "        'Latitude': latitude,\n",
    "        'Longitude': longitude\n",
    "    }\n",
    "    return parsed_listing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell calls the parsing function on the massive list of urls, organizing resulting dfs into a dictionary with city names as keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# Create an empty DataFrame to store the extracted data\n",
    "columns = ['Make', 'Model', 'Year', 'Miles', 'Price', 'Title', 'Paint', 'Drive', 'Condition', 'Description', 'Latitude', 'Longitude']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Create an empty dictionary to hold DataFrames for each city\n",
    "city_df_dict = {}\n",
    "\n",
    "for city, urls in city_url_dict.items():\n",
    "    # Initialize an empty DataFrame for each city\n",
    "    df = pd.DataFrame(columns=['Make', 'Model', 'Year', 'Miles', 'Price', 'Title', 'Paint', 'Drive', 'Condition', 'Description', 'Latitude', 'Longitude'])\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            script_tag = soup.find('script', {'id': 'ld_posting_data'})\n",
    "\n",
    "            if script_tag is None:\n",
    "                print(f\"Skipping URL {url} - script_tag not found\")\n",
    "                continue\n",
    "\n",
    "            json_data = json.loads(script_tag.string)\n",
    "\n",
    "            # Extract relevant fields via dictionaries which are outputs from functions above\n",
    "            parsed_data = parse_attrgroup(soup)\n",
    "            parsed_listing = parse_ld_posting_data(soup)\n",
    "\n",
    "            # Append the extracted data to the DataFrame\n",
    "            new_row = pd.DataFrame({\n",
    "                'Make': [parsed_data['Make']],\n",
    "                'Model': [parsed_data['Model']],\n",
    "                'Year': [parsed_data['Year']],\n",
    "                'Miles': [parsed_data['Odometer']],\n",
    "                'Price': [parsed_listing['Price']],\n",
    "                'Title': [parsed_data['Title Status']],\n",
    "                'Paint': [parsed_data['Paint Color']],\n",
    "                'Drive': [parsed_data['Drive']],\n",
    "                'Condition': [parsed_data['Condition']],\n",
    "                'Description': [parsed_listing['Description']],\n",
    "                'Latitude': [parsed_listing['Latitude']],\n",
    "                'Longitude': [parsed_listing['Longitude']],\n",
    "            })\n",
    "\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "            \n",
    "        except AttributeError as e:\n",
    "            print(f\"Skipping URL {url} - Error: {e}\")\n",
    "\n",
    "    # Save the DataFrame for this city to the dictionary\n",
    "    city_df_dict[city] = df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "craigslist-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
